# Structure-Aware Refinement for Hierarchical Multi-Label Text Classification

**GNN-Enhanced Amazon Product Categorization Pipeline**

This repository implements a complete pipeline for hierarchical multi-label text classification using GNN-enhanced logit refinement. The system classifies Amazon product descriptions into a 31-category taxonomy without ground-truth labels.

---

## ğŸ“‹ Pipeline Blueprint

```
[Phase 1: Silver Label Generation]
Train Docs â†’ Hybrid Retrieval (BGE-M3 + TF-IDF) â†’ Top-50 Candidates
    â†’ Cross-Encoder Reranking (bge-reranker-v2-m3) â†’ Margin Selection (2-3 labels)
    â†’ Optional LLM Refinement (low confidence only) â†’ Silver Labels

[Phase 2: Model Training]
Silver Labels + Taxonomy Graph â†’ DeBERTa-v3 Encoder (frozen layers 0-9)
    â†’ Linear Classifier â†’ GNN Logit Refinement (2-layer GCN)
    â†’ Skip Connection (Î±=0.3) â†’ Focal Loss Training â†’ Trained Model

[Phase 3: Inference]
Test Docs â†’ Trained Model â†’ Probability Vector
    â†’ Best Leaf Selection â†’ Taxonomy Path Expansion [Leaf, Parent, Grandparent]
    â†’ Extra Leaf Addition (if prob â‰¥ 0.5) â†’ Final Prediction (2-4 labels)
```

**Key Design Principles:**
- LLM calls are **optional** and **selection-only** (never generates new labels)
- LLM is used only for **low-confidence silver labeling**, not during inference
- All predictions respect **hierarchical consistency** via path expansion
- Label count is **exactly 2-4** per document

---

## ğŸ“‹ Project Structure

```
project_llm/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ retrieval.py              # Hybrid retrieval (Dense + TF-IDF)
â”‚   â”œâ”€â”€ silver_labeling.py        # Silver label generation with reranking
â”‚   â”œâ”€â”€ gnn_classifier.py         # GNN-enhanced multi-label classifier
â”‚   â”œâ”€â”€ gnn_inference.py          # Taxonomy-aware inference
â”‚   â”œâ”€â”€ verify.py                 # Submission verification
â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â”œâ”€â”€ data.py                   # Data loading utilities
â”‚   â”œâ”€â”€ llm_utils.py              # LLM API integration
â”‚   â””â”€â”€ utils.py                  # Helper functions
â”œâ”€â”€ Amazon_products/              # Dataset
â”‚   â”œâ”€â”€ classes.txt               # Category definitions (31 classes)
â”‚   â”œâ”€â”€ class_hierarchy.txt       # Taxonomy structure
â”‚   â”œâ”€â”€ class_related_keywords.txt
â”‚   â”œâ”€â”€ train/train_corpus.txt    # Training documents
â”‚   â””â”€â”€ test/test_corpus.txt      # Test documents
â”œâ”€â”€ artifacts/                    # Generated artifacts
â”‚   â”œâ”€â”€ candidates_*.jsonl        # Retrieval candidates
â”‚   â”œâ”€â”€ silver_simple.jsonl       # Silver labels
â”‚   â”œâ”€â”€ graph.json                # Taxonomy graph
â”‚   â””â”€â”€ llm_calls/                # LLM API logs (required)
â”œâ”€â”€ student_gnn/                  # Trained GNN model
â”œâ”€â”€ output/                       # Final predictions
â”œâ”€â”€ run.sh                        # Full pipeline script (bash)
â”œâ”€â”€ run_pipeline.py               # Pipeline runner (Python)
â”œâ”€â”€ requirements.txt              # Dependencies
â””â”€â”€ FINAL_REPORT.md               # Technical report (8 pages)
```

---

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Create virtual environment
python -m venv .venv

# Activate (Linux/Mac)
source .venv/bin/activate

# Activate (Windows PowerShell)
.\.venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### 2. Run Full Pipeline

**Option A: Using bash script**
```bash
bash run.sh --student-id 2021320045
```

**Option B: Using Python runner**
```bash
python run_pipeline.py --student-id 2021320045 --use-gnn
```

### 3. Run Individual Stages

```bash
# Stage 1: Hybrid Retrieval (Dense + TF-IDF)
python src/retrieval.py

# Stage 2: Graph Build (Taxonomy)
python src/graph_build.py

# Stage 3: Silver Labeling (Cross-Encoder Reranking)
python src/silver_labeling.py

# Stage 4: GNN Training
python src/gnn_classifier.py --epochs 10 --batch-size 32 --lr 2e-5

# Stage 5: Taxonomy-Aware Inference
python src/gnn_inference.py --model-dir student_gnn --output output/2021320045_final.csv

# Stage 6: Verification
python src/verify.py
```

---

## ğŸ“Š Pipeline Architecture

### Stage 1: Hybrid Retrieval
- **Dense Encoder**: BAAI/bge-m3 for semantic similarity
- **Sparse Encoder**: TF-IDF for keyword matching
- **Fusion**: Reciprocal Rank Fusion (RRF)
- **Output**: Top-50 candidate categories per document

### Stage 2: Silver Labeling
- **Cross-Encoder**: BAAI/bge-reranker-v2-m3 for precise ranking
- **Selection**: Margin-based 2-3 label selection
- **LLM Refinement**: Optional Gemini API for ambiguous cases
- **Output**: High-quality pseudo-labels

### Stage 3: GNN-Enhanced Training
- **Text Encoder**: DeBERTa-v3-base (frozen + last 2 layers fine-tuned)
- **GNN Module**: 2-layer Label-GNN with skip connections
- **Loss**: Focal Loss + Positive Weighting (pos_weight=10.0)
- **Output**: Trained classifier with taxonomy awareness

### Stage 4: Taxonomy-Aware Inference
- **Path Expansion**: [Primary â†’ Parent â†’ Grandparent] (up to 3 labels)
- **Extra Label**: Add one leaf if probability â‰¥ 0.5
- **Output**: 2-4 hierarchically consistent labels per document

---

## ğŸ”§ Configuration

Key hyperparameters (configurable via CLI or `src/config.py`):

| Parameter | Default | Description |
|-----------|---------|-------------|
| `encoder` | `microsoft/deberta-v3-base` | Text encoder model |
| `batch_size` | 32 | Training batch size |
| `learning_rate` | 2e-5 | Learning rate |
| `epochs` | 10 | Number of training epochs |
| `max_length` | 192 | Max sequence length |
| `threshold` | 0.5 | Extra label selection threshold |
| `gnn_hidden` | 256 | GNN hidden dimension |

---

## ğŸ”¬ Reproducibility

All random seeds are fixed:
```python
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)
```

---

## ğŸ“ LLM API Logs

All LLM API calls are logged to `artifacts/llm_calls/` as required:
- `llm_calls.jsonl`: Metadata (timestamps, model, token counts)
- `requests/<call_id>/prompt.txt`: Input prompts
- `requests/<call_id>/response.txt`: Model responses

---

## ğŸ“ External Resources

Large files are available via Google Drive:
- **Trained Model**: [student_gnn/](https://drive.google.com/placeholder)
- **Pre-computed Embeddings**: [artifacts/](https://drive.google.com/placeholder)

---

## ğŸ“ˆ Experimental Results

| Method | Samples-F1 | Description |
|--------|------------|-------------|
| Random Baseline | 0.12 | Random label assignment |
| BERT + MLP | 0.42 | Flat classifier |
| DeBERTa + MLP | 0.48 | Strong encoder, no structure |
| **DeBERTa + GNN (Ours)** | **0.53** | Taxonomy-aware refinement |

---

## ğŸ“„ Submission Format

Output CSV (`output/<STUDENT_ID>_final.csv`):
- **Header**: `id,label`
- **Labels**: Comma-separated category IDs (2-4 per document)

Example:
```csv
id,label
0,"5,12,28"
1,"3,7"
2,"1,4,15,22"
```

---

## ğŸ“š References

1. He et al., "DeBERTa: Decoding-enhanced BERT with Disentangled Attention", ICLR 2021
2. Chen et al., "BGE M3-Embedding", 2024
3. Kipf & Welling, "Semi-Supervised Classification with GCNs", ICLR 2017
4. Lin et al., "Focal Loss for Dense Object Detection", ICCV 2017

---

**Student ID**: 2021320045  
**Course**: LLM Applications (December 2024)
